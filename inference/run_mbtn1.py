# -*- coding: utf-8 -*-
"""run_mbtn1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sNetMF-3B3RWQfhPgDUF1gf8tkc0zpv-
"""

!ls

!git clone https://huggingface.co/phanikartcs/mahabharata_tatparya_nirnaya1

# Commented out IPython magic to ensure Python compatibility.
# %cd mahabharata_tatparya_nirnaya1/mahabharata_tatparya_nirnaya_model1

!ls

!pip install -q transformers accelerate peft bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

base_model = "Qwen/Qwen1.5-1.8B"
lora_model = "/content/mahabharata_tatparya_nirnaya1/mahabharata_tatparya_nirnaya_model1"

tokenizer = AutoTokenizer.from_pretrained(base_model)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    device_map="auto",
    load_in_4bit=True
)

model = PeftModel.from_pretrained(model, lora_model)

def build_prompt(instruction):
    return f"""### Instruction:
{instruction}

### Response:
"""

prompt = build_prompt("Who killed Vali?")
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=256,
    do_sample=True,
    temperature=0.1,
    eos_token_id=tokenizer.eos_token_id
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))