{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCLyvAHzGyYE",
        "outputId": "0af81572-d764-4860-c333-0d4e68c3fcb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "r-GEoysox2mH",
        "outputId": "1caa819e-0447-4e7d-a6de-1dad1b1214fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mahabharata_tatparya_nirnaya1'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 46 (delta 5), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (46/46), 1.74 MiB | 2.57 MiB/s, done.\n",
            "Filtering content: 100% (20/20), 131.24 MiB | 73.20 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/phanikartcs/mahabharata_tatparya_nirnaya1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mahabharata_tatparya_nirnaya1/mahabharata_tatparya_nirnaya_model1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7LSo3isWylKZ",
        "outputId": "9454d445-c5d4-474c-bb4c-49c3e044547c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mahabharata_tatparya_nirnaya1/mahabharata_tatparya_nirnaya_model1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cU8fQrGHHE1",
        "outputId": "abc5fc18-e1fc-461d-e190-6b7d70c67921"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adapter_config.json\t   checkpoint-2542   special_tokens_map.json\n",
            "adapter_model.safetensors  checkpoint-3813   tokenizer_config.json\n",
            "added_tokens.json\t   merges.txt\t     tokenizer.json\n",
            "chat_template.jinja\t   README.md\t     vocab.json\n",
            "checkpoint-1271\t\t   requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate peft bitsandbytes"
      ],
      "metadata": {
        "id": "YawllkTozAqs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "rOWBxFaD2Vzm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"Qwen/Qwen1.5-1.8B\"\n",
        "lora_model = \"/content/mahabharata_tatparya_nirnaya1/mahabharata_tatparya_nirnaya_model1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRLxwzdG3KU1",
        "outputId": "4588a398-7e09-4889-f5ad-1c4d93697cb3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(model, lora_model)"
      ],
      "metadata": {
        "id": "zuokA-Qt5MLr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(instruction):\n",
        "    return f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "prompt = build_prompt(\"Who killed Vali?\")\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4D-58x15FhD",
        "outputId": "2caacfe3-3eea-443c-e0b8-d970c228dbf5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Who killed Vali?\n",
            "\n",
            "### Response:\n",
            "Bhima\n"
          ]
        }
      ]
    }
  ]
}