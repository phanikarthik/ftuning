# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10LZ3TSdDHuY6-um8daJIC0-E1LQRVqOq
"""

# ==============================
# 1. Setup Environment
# ==============================
!pip install -q transformers datasets peft accelerate bitsandbytes

!pip install huggingface_hub

!huggingface-cli login

import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model

# ==============================
# 2. Load Your Dataset
# ==============================
# Upload your qa_results_alpaca.jsonl to Colab first (Files panel -> Upload)
INPUT_FILE_NAME = '/content/aplaca_op.jsonl'
dataset = load_dataset("json", data_files=INPUT_FILE_NAME)
dataset = dataset["train"]   # select the split

print(dataset)  # sanity check

# ==============================
# 3. Choose Base Model
# ==============================
# Pick a small model if GPU memory is limited (like 1-2B)
# Recommended options for Colab T4 / A100:
# "meta-llama/Llama-2-7b-hf"  (needs A100 or better)
# "NousResearch/Llama-2-7b-hf" (same as above)
# "mistralai/Mistral-7B-v0.1"  (lighter)
# "Qwen/Qwen1.5-1.8B"          (fits easily on Colab T4)
# We'll default to a smaller one here:

model_name = "Qwen/Qwen1.5-1.8B"

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    device_map="auto",
    low_cpu_mem_usage=True
)

# ==============================
# 4. Prepare LoRA Config
# ==============================
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # for LLaMA/Qwen-style models
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# ==============================
# 5. Tokenization Function
# ==============================
# Format prompt (like Alpaca)
# Format prompt
def format_prompt(example):
    if example["input"]:
        prompt = f"### Instruction:\n{example['instruction']}\n\n### Input:\n{example['input']}\n\n### Response:\n"
    else:
        prompt = f"### Instruction:\n{example['instruction']}\n\n### Response:\n"
    return prompt

# Tokenize
def tokenize(example):
    prompt = format_prompt(example)
    full_text = prompt + example["output"]

    tokenized = tokenizer(full_text, truncation=True, padding="max_length", max_length=512)

    # Labels
    labels = tokenized["input_ids"].copy()
    prompt_len = len(tokenizer(prompt)["input_ids"])
    labels[:prompt_len] = [-100] * prompt_len  # ignore prompt tokens

    tokenized["labels"] = labels
    return tokenized

# ==============================
# 6. Training Arguments
# ==============================
training_args = TrainingArguments(
    output_dir="./alpaca-ft",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    fp16=True,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=10
)

# ==============================
# 7. Trainer
# ==============================

tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

trainer.train()

# ==============================
# 8. Save Model
# ==============================
model.save_pretrained("./alpaca-ft")
tokenizer.save_pretrained("./alpaca-ft")

# ==============================
# 9. Inference Test
# ==============================
from transformers import pipeline
prompt = """### Instruction:
What does the essay define consciousness as?

### Response:
"""

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))